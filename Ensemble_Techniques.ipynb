{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Ensemble Techniques Theoretical Answers:-\n",
        "---\n",
        "---\n",
        "\n",
        "###1. Can we use Bagging for regression problems?\n",
        "Yes, Bagging can be used for regression as well as classification problems. For regression, Bagging aggregates predictions using average instead of majority voting.\n",
        "\n",
        "---\n",
        "###2. What is the difference between multiple model training and single model training?\n",
        "- Single model training uses one algorithm to learn patterns.\n",
        "\n",
        "- Multiple model training (ensemble) combines several models to improve accuracy and reduce overfitting/variance.\n",
        "\n",
        "---\n",
        "###3. Explain the concept of feature randomness in Random Forest.\n",
        "In Random Forest, each tree chooses a random subset of features for splitting nodes. This feature randomness increases model diversity and reduces correlation between trees.\n",
        "\n",
        "---\n",
        "###4. What is OOB (Out-of-Bag) Score?\n",
        "OOB Score is the validation score calculated using data not included in the bootstrap sample for training a tree. It provides an internal estimate of model performance without using a separate validation set.\n",
        "\n",
        "---\n",
        "###5. How can you measure the importance of features in a Random Forest model?\n",
        "Feature importance is measured by evaluating how much each feature decreases impurity (Gini/entropy) across all trees. You can access this via .feature_importances_ in sklearn.\n",
        "\n",
        "---\n",
        "###6. Explain the working principle of a Bagging Classifier.\n",
        "- Creates multiple models (e.g., decision trees) using bootstrap samples.\n",
        "\n",
        "- Each model makes a prediction.\n",
        "\n",
        "- Final prediction is made by majority voting (classification).\n",
        "\n",
        "---\n",
        "###7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "Use metrics like accuracy, precision, recall, F1-score, and confusion matrix on a test set or through cross-validation/OOB score.\n",
        "\n",
        "---\n",
        "###8. How does a Bagging Regressor work?\n",
        "Similar to Bagging Classifier, but instead of voting, it averages predictions from multiple base regressors trained on different bootstrap samples.\n",
        "\n",
        "---\n",
        "###9. What is the main advantage of ensemble techniques?\n",
        "They increase accuracy, reduce overfitting, and improve generalization by combining diverse models.\n",
        "\n",
        "---\n",
        "###10. What is the main challenge of ensemble methods?\n",
        "They can be computationally expensive, harder to interpret, and may lead to complexity in deployment.\n",
        "\n",
        "---\n",
        "###11. Explain the key idea behind ensemble techniques.\n",
        "Ensemble learning combines the strengths of multiple models to produce better performance than any single model.\n",
        "\n",
        "---\n",
        "###12. What is a Random Forest Classifier?\n",
        "It is an ensemble of decision trees built using bagging and feature randomness, and it predicts using majority voting.\n",
        "\n",
        "---\n",
        "###13. What are the main types of ensemble techniques?\n",
        "- Bagging (e.g., Random Forest)\n",
        "\n",
        "- Boosting (e.g., AdaBoost, XGBoost)\n",
        "\n",
        "- Stacking (combining models using a meta-model)\n",
        "\n",
        "---\n",
        "###14. What is ensemble learning in machine learning?\n",
        "A technique where multiple models (weak or strong) are combined to produce a more accurate and robust model.\n",
        "\n",
        "---\n",
        "###15. When should we avoid using ensemble methods?\n",
        "- When interpretability is crucial.\n",
        "\n",
        "- When the dataset is small.\n",
        "\n",
        "- When a single model performs well already.\n",
        "\n",
        "---\n",
        "###16. How does Bagging help in reducing overfitting?\n",
        "By training on different random samples of data, Bagging reduces variance, thus preventing overfitting common in high-variance models like decision trees.\n",
        "\n",
        "---\n",
        "###17. Why is Random Forest better than a single Decision Tree?\n",
        "Because it reduces overfitting, improves accuracy, and is more robust to noise, due to ensemble of multiple trees and random feature selection.\n",
        "\n",
        "---\n",
        "###18. What is the role of bootstrap sampling in Bagging?\n",
        "Bootstrap sampling creates diverse training datasets by sampling with replacement. This helps in training diverse models, which is crucial for reducing variance.\n",
        "\n",
        "---\n",
        "###19. What are some real-world applications of ensemble techniques?\n",
        "- Medical diagnosis (cancer detection)\n",
        "\n",
        "- Credit scoring in finance\n",
        "\n",
        "- Spam detection\n",
        "\n",
        "- Recommendation systems\n",
        "\n",
        "- Stock market prediction\n",
        "\n",
        "---\n",
        "###20. What is the difference between Bagging and Boosting?\n",
        "| Feature        | Bagging            | Boosting                    |\n",
        "| -------------- | ------------------ | --------------------------- |\n",
        "| Model Training | Parallel           | Sequential                  |\n",
        "| Focus          | Reduces variance   | Reduces bias                |\n",
        "| Data Sampling  | Bootstrap sampling | Weighted sampling           |\n",
        "| Examples       | Random Forest      | AdaBoost, XGBoost, LightGBM |\n",
        "\n",
        "---\n",
        "---\n",
        "# Practical Answers:-\n",
        "\n",
        "---\n",
        "---\n",
        "###21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TzBJF5xe6mql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dw5vlPu6aQS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "OFLXH1wH4VQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "24MUgqiW4ZMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "vc5FQCYX4byv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "for name, importance in zip(load_breast_cancer().feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "lFRVKevb4fL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "f-4x0_vb4ulO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, test_size=0.3, random_state=42)\n",
        "tree = DecisionTreeRegressor()\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "tree.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "tree_pred = tree.predict(X_test)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree RÂ² Score:\", r2_score(y_test, tree_pred))\n",
        "print(\"Random Forest RÂ² Score:\", r2_score(y_test, rf_pred))"
      ],
      "metadata": {
        "id": "u0KRAzue4x8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "gCf3kQRS5KrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, test_size=0.3, random_state=42)\n",
        "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_oob.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", rf_oob.oob_score_)"
      ],
      "metadata": {
        "id": "OKLsgsZt5OU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###26. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "rl6Xz4gz6K4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "bag_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bag_svm.fit(X_train, y_train)\n",
        "y_pred = bag_svm.predict(X_test)\n",
        "print(\"Bagging SVM Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "w5L7BnQE6NuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "hSgGBtQB6RMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [10, 50, 100, 200]:\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    print(f\"{n} Trees: Accuracy = {acc:.4f}\")"
      ],
      "metadata": {
        "id": "qdWWQj4L6VXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n"
      ],
      "metadata": {
        "id": "1TI1MDbJ6XvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X_bin, y_bin = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "bag_log = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "bag_log.fit(X_train, y_train)\n",
        "y_prob = bag_log.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "id": "ORbAoKoY6d6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###29. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "EOkZ9Dv86hxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "importances = rf_reg.feature_importances_\n",
        "\n",
        "for name, importance in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "W1t9ekT26k2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "yODDYnAG6nch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "bag_acc = bag.score(X_test, y_test)\n",
        "rf_acc = rf.score(X_test, y_test)\n",
        "\n",
        "print(\"Bagging Accuracy:\", bag_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)"
      ],
      "metadata": {
        "id": "m8E1Ncal6rdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "lUmHiHXo8D8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "SppIKvJB8Kak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###32. Bagging Regressor with different numbers of base estimators."
      ],
      "metadata": {
        "id": "70xvlOKf8UIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [10, 50, 100]:\n",
        "    model = BaggingRegressor(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "    print(f\"{n} Estimators - MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "2A2sa64W8aS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###33. Random Forest Classifier: Analyze misclassified samples."
      ],
      "metadata": {
        "id": "STDoDj9u8lT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "misclassified = X_test[y_pred != y_test]\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified))\n"
      ],
      "metadata": {
        "id": "XJLcOrjc8rDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "FrKxglR89EQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "bag = BaggingClassifier(base_estimator=dt, n_estimators=100, random_state=42)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt.score(X_test, y_test))\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "uoK-vEFa9MRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "Fbl0GwXS9PCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)"
      ],
      "metadata": {
        "id": "_9Uo4czU9UsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "weffowsC9ZmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=[('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True)), ('lr', LogisticRegression(max_iter=1000))],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "mt2lLg9o9gdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "LSRM9AVz9lDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "for i in range(5):\n",
        "    print(f\"{data.feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
      ],
      "metadata": {
        "id": "70FfcIr59v53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "Xnsm8QER9z_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "bag = BaggingClassifier(n_estimators=100, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred = bag.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "UdqcINdt94uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "zXZWBsUS97Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for d in [3, 5, 10, None]:\n",
        "    rf = RandomForestClassifier(max_depth=d, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"max_depth={d} -> Accuracy: {rf.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "id": "TrabO1uj-A4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance."
      ],
      "metadata": {
        "id": "QBbyYMPw-EOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "for model in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    name = model.__class__.__name__\n",
        "    bag = BaggingRegressor(base_estimator=model, n_estimators=10, random_state=42)\n",
        "    bag.fit(X_train, y_train)\n",
        "    print(f\"{name} - MSE:\", mean_squared_error(y_test, bag.predict(X_test)))"
      ],
      "metadata": {
        "id": "Hxn0M1pZ-Koe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "omMoF_ta-Nom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "id": "oBCjd3wc-Sw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###42. Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "wOnwBc-V-ZwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "bag = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "scores = cross_val_score(bag, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-Val Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "id": "JjBl-rKs-gSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###43. Train a Random Forest Classifier and plot the Precision-Recall curve."
      ],
      "metadata": {
        "id": "trdUIcvc-jVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "probs = rf.predict_proba(X_test)[:, 1]\n",
        "prec, rec, _ = precision_recall_curve(y_test, probs)\n",
        "\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDv6G1xn-pXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "tded7RPm-scu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack2 = StackingClassifier(\n",
        "    estimators=[('rf', RandomForestClassifier()), ('lr', LogisticRegression(max_iter=1000))],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "stack2.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack2.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "ETcbV2hb-wys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "w_oRL2T--ze1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for b in [True, False]:\n",
        "    reg = BaggingRegressor(n_estimators=10, bootstrap=b, random_state=42)\n",
        "    reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, reg.predict(X_test))\n",
        "    print(f\"Bootstrap={b} -> MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "2L-Y1rbO-3s2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}